---
title: "한국어 마크다운 검색엔진 개발 기록"
date: "2024-05-07"
author: "bill0077"
---

블로그를 쓰다보니 기존의 글을 참고하는 경우가 자주 생겼다. 그런데 이럴때마다 동일한 내용을 찾는데 클릭 여러번으로 찾아가는 과정이 꽤나 번거롭다. 그래서 간단한 검색기능을 추가해주기로 했다. 한국어 마크다운 문서 기반의 검색 엔진을 만들고 블로그에 직접 적용해보자.

또한 평소에 다른 블로그들에서 내용을 검색할때는 검색어와 내용의 글자가 정확히 일치해야만 검색 결과를 반환한다는 점이 불편했다. 약간의 오타 정도는 스스로 교정할 수 있는 검색 기능을 목표로 구현해보겠다.

# 문제 상황
먼저 문제 상황을 정리해보자. 서버엔 검색을 적용할 여러개의 마크다운 문서가 저장되어있고, 검색 엔진으로부터 사용자가 입력하는 검색어(=query)가 주어진다. 이때 query에 맞춰 적합한 마크다운을 순서대로 정렬해서 링크를 띄워주면 검색 과정이 완료될 것이다.

이를 위해 먼저 query와 문서를 공백을 기준으로 쪼갠다 (query를 통짜로 사용하면 문서에서 대응되는 단어가 거의 없을 것이다). 이때 쪼개진 query나 문서의 단어 한조각을 token이라고 하겠다. 간단히 생각해보면 각 검색어 token이 각 문서에서 몇번이나 발견되는지 개수를 기준으로 문서를 정렬하면 될 것 같다. **그런데..**

# 문자열 군집화
이때 문제는 영어와 다르게 한국어는 단어 끝마다 조사가 붙어 사실상 같은 token이라도 다르게 처리된다는 것이다. 예를들어 '컨테이너'라고 검색했을 때 인간은 '컨테이너가', '컨테이너에서' 와 같은 단어도 '컨테이너'를 지칭하는 것을 쉽게 알 수 있지만, 알고리즘 상으로 이를 구현하기 쉽지 않다. (자연어 AI를 사용해 각 token을 embedding하지 않는 이상.. 하지만 이 경우 검색 시간 증가도 감수해야하고 AI를 학습시킬 데이터를 만들기도 여간 귀찮은게 아니다)

그래서 가장 먼저 생각해낸 것은 문서별로 유사한 token를 하나의 그룹으로 묶어내는 것이다. Levenshtein distance를 활용해 하나의 token이 다른 token에 얼마나 가까운가의 지표로 활용했다. 모든 token의 순서쌍에 대해 아래와 같은 지표를 계산해 해당 지표의 평균이 0.2(0.2는 시행착오로 찾은 적절한 임의의 임계값이다.)보다 작으면 하나의 그룹인 것으로 간주하고 그렇지 않으면 다른 그룹으로 간주한다.
```
Levenshtein_distance(token1, token2) / average_length(token1, token2)
```

이 방법의 가장 큰 단점은 인덱스 생성에 걸리는 시간이다! token 하나의 평균 길이를 $n$이라 가정하면 Levenshtein distance는 $O(n^2)$의 시간 복잡도가 걸린다. 마크다운 문서가 평균적으로 $m$개의 token이 있다면 처음 token은 0개, 그다음 token은 1개, 이런식으로 마지막 token은 m개 token과 비교를 하게 되므로 $O(n^2m^2)$의 시간복잡도를 갖는다. 마크다운 문서 전체의 길이 $L=nm$이므로 $O(L^2)$ 라고도 볼 수 있다.

어디에서 시간을 절약할 수 있을까? 이 알고리즘의 가장 큰 문제는 각 token 그룹 내부의 모든 token과 각각 비교한다는 것이다. token 그룹 전체를 대표하는 하나의 대표 token을 만들고 해당 대표만 비교에 이용하면 어떨까? 이 경우 각 token은 그룹별로만 비교하면 되므로 그룹의 개수 $g$에 대해 $O(n^2g^2)$이 걸린다.
<center>
<img src="___MEDIA_FILE_PATH___/tokens-per-groups-graph.png" width="80%" title="tokens-per-groups-graph"/>
</center>

기존에 있던 24개의 문서들로 시험해본 결과 위의 그래프처럼 $g \simeq 0.43m$ 정도이므로 기존에 걸리던 시간 대비 0.185배이며 5.4배가량 성능 향상을 이루어낼 수 있다. 시간복잡도 자체의 향상은 아니지만 그래도 꽤나 큰 진전이다.

다음으로 대표 token을 어떻게 생성할지 구상해보자. 처음에는 LCS 알고리즘을 사용할까 했지만 이러면 점점 token이 짧아져 문제가 생길것 같았고, 전체 token 그룹을 표현하지 못하는 단점이 있다. 결론적으로 구상한 대표 token(=centroid라 하겠다)의 생성 방식은 대강 아래의 방식을 따른다.
```
centroid[i] = max(token[i] for every token in group)
```

그런데 이 방식을 택했을 때 대표를 정하는 과정에서 예상치 못한 오류가 생겨났다. 예를들어 `content`가 `context` token보다 조금 더 많이 쓰이는 경우 둘중 `content` 하나로 대표가 정해져 `context`를 검색하려 해도 검색이 제대로 안되는 상황이 일어난다. 따라서 centroid가 얼마나 정확한지를 centrality를 추가로 고려해 Levenshtein distance를 구할 때 반영해주었다. token과 centroid를 비교해서 centrality가 큰 문자가 다를 경우에는 차이가 크다고 계산하고, centrality가 작은 문자는 달라도 그 차이가 적다고 계산하는 식이다. 이런식으로 위의 문제를 대다수 해결할 수 있었다.
```
centrality[i] = count(max_char[i]) / count(token)
```

**ps** 시간 복잡도를 혁신적으로 줄이지 못한 것은 꽤나 아쉽지만, 지금은 본래의 목표에만 집중하고 성능 향상은 다음에 제대로 생각해보자.

# tokenization
앞서 token을 만들 때 단순히 공백을 기준으로 만든다고 했지만, 실제로 그렇게 적용해본 결과 여러 문제점이 있었다. 대표적으로
1. `[Backend]`의 형태처럼 특수 문자가 포함된 token이 생성 된다. 검색할때 특수문자는 대체로 포함하지 않기에(적어도 나는 그렇다) 오차가 생길 수 있다.
2. 외부 링크처럼 너무 길면서도 실제로 검색될 확률은 적은 token이 성능 저하를 일으킨다 (심지어 개수도 꽤나 많다).
3. `http/https`처럼 분리되는게 좋아보이는 token들이 하나로 취급된다.

위 문제를 해결하는 가장 간단한 방법은 모든 특수문자를 기준으로 token을 쪼개는 것이었다. 실제로 위의 문제들은 모두 해결되었고, 덤으로 index를 생성하는 시간도 기존에 비해 1.5배 가량 향상되었다 (토큰 개수가 살짝 늘어났지만 평균 토큰 길이가 짧아졌기 때문).
<center>
<img src="___MEDIA_FILE_PATH___/time-per-tokens-graph.png" width="80%" title="time-per-tokens-graph"/>
</center>

그런데 이런 경우 반대로 `react-markdown`, `ENV_PATH` 같은 하나로 있는게 좋아보이는 token 또한 여러개로 쪼개진다는 단점이 있다. 이런 경우는 오래 생각해 보았지만 일반적으로 적용가능한 방법이 딱히 없는것 같다 (각 token들을 학습하는 AI를 만들지 않는 이상..). 그래서 단순히 `-`, `_`로 이어진 token은 쪼개지 않는 것으로 처리했다.

**ps** 아쉬운 점은 이렇게 할 경우에도 여전히 `cert bot`이라는 query를 `certbot`이라는 token으로 연결짓지는 못한다는 것이다. 추후에 여러 token을 조합해서 가장 적합한 token을 검색에 활용하는 구조를 추가로 고안해봐도 좋을 것 같다.

# 한국어 유니코드 변한
추가적으로 token을 전처리하는 과정을 추가했다. 먼저 영문 token은 대소문자를 구분하지 않는것이 indexing이나 검색 결과 측면에서 더 나았기에 모두 소문자로 변환해 주었다. 또한 한국어 토큰은 각 유니코드 문자를 키보드에 입력되는 순열로 처리하였다. 예를들어 기존 유니코드를 그대로 token으로 사용하면 `컨테이너`는 `컨텡ㅣ너`와 2글자가 차이나지만, 키보드에 입력되는 순열로 보면 모두 `ㅋㅓㄴㅌㅔㅇㅣㄴㅓ`로 같기 때문에 동일하게 처리된다. 한국어는 작은 오타 하나가 전체 유니코드를 모두 바꾸는 경우가 많아 이게 오타 잡기에 더욱 효과적이라 생각한다. 

한글 유니코드는 아래와 같은 규칙을 따른다. 
```
한글코드의 값 = ((초성 * 21) + 중성) * 28 + 종성 + 0xAC00
```

이 과정의 코드는 reference의 소스(https://github.com/neotune/python-korean-handler/blob/master/korean_handler.py )를 많이 참조했고, 쌍자음이나 겹받침을 각각의 키보드 입력으로만 따로 분리해 주었다.

ex) ㄲ -> shift+ㄱ, ㄵ -> ㄴ+ㅈ

# 한영키 오타 감지
마지막으로 한영키 오타로 언어가 변환되지 않은채 발생한 오타를 감지하기 위해 검색어 token이 다른 모든 token과 유사도가 떨어질때 이를 한영키 오타로 판단하고 언어가 변환된 버전으로 token을 바꿔 다시 검색해주었다. 위에서 한국어를 유니코드에서 키보드 입력으로 변형하였기에 변환 과정 자체는 간단했다.

# 검색 적합도 함수
이제 적절한 token이 준비되었다. 각 문서별로 어떤 token이 몇개가 있는지 정리되었고, 검색어 token이 각 token과 얼마나 유사한지도 계산할 수 있다. 이때 어떤 문서를 가장 적합하다고 판별해야할까? 

1. **score = count / (0.01+distance)^2**  
가장 먼저 생각한 것은 단순히 가장 유사한 단어의 개수(=count)를 해당 단어와의 차이(=distance)의 제곱으로 나누어 주는 것이었다 (divide by 0를 회피하기 위해 0.01 더해줌). 이때 문제는 검색어와 token이 완전히 일치할 때는 상관없지만 한 글자라도 차이가 나면 token의 개수가 너무 영향이 크다는 것이다. `certbot`를 검색하려다 `cretbot`을 입력하면 유사도가 비슷한 `certbot`(차이 2)보다 token 개수가 훨씬 많은 `create`(차이 4)를 검색하는 식이다.

2. **score = log(1+count) / (0.01+distance)^2**  
그래서 count에 log(1+count)를 적용했지만 여전히 count의 영향이 너무 컸다.

3. **score =  (2 ^ (-15*distance)) * log(1+count)**  
결론적으로 distance의 중요도를 극단적으로 높이되 같은 유사도라면 count에 영향을 받는 방식으로 검색 적합도 함수를 작성했다.

**ps** 지금은 기본적인 token 개수만을 고려했지만 조금만 더 생각해보면 훨씬 나은 방법이 나을 것 같다. 이부분도 추후에 보완해보겠다.

# 결론
<center>
<img src="___MEDIA_FILE_PATH___/search-result.png" width="100%" title="result-demo"/>
</center>
`dockr 컨ㅌ이ㅓㄴ`, `docker zjsxpdlsj`라고 입력해도 모두 `docker 컨테이너`로 잘 검색되는 모습. 입력된 검색어에 맞춰 상위에는 모두 검색어인 `docker 컨테이너`와 잘 어울리는 결과가 제시되었다 (이번 글에서 컨테이너라는 단어를 너무 많이 써서 이제는 이 문서가 가장 먼저 뜰 것 같다).

위의 과정을 모두 거쳐 꽤나 만족스러운 검색 엔진을 만들어낸 것 같다. 무엇보다 평소 블로그 검색은 구글 검색처럼 자동 한영 변환이나 오타 수정이 안되는 경우가 많아서 불편하다고 생각하고 있었는데 이런 부분을 수정한 버전을 만들어낸 것이 뿌듯하다. 완성된 코드는 아래의 repo에서 볼 수 있다.

https://github.com/bill0077/kor-markdown-search-engine

TODO: 검색 적합도 함수 개선, 시간복잡도 개선, 최적 token 추론 기능 추가

# reference
wikipedia, Levenshtein distance: https://en.wikipedia.org/wiki/Levenshtein_distance

neotune, python-korean-handler: https://github.com/neotune/python-korean-handler/blob/master/korean_handler.py