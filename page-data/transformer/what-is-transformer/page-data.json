{"componentChunkName":"component---src-templates-post-js","path":"/transformer/what-is-transformer/","result":{"pageContext":{"markdown":"---\r\ntitle: \"transformer이란 무엇인가\"\r\ndate: \"2024-04-20\"\r\nauthor: \"bill0077\"\r\n---\r\n\r\n최근 LLM들이 엄청난 강세이다. 이러한 모델들의 자연어 처리가 비약적으로 상승한 것에는 transformer이라는 아키텍쳐가 개발된 것에 있다. 이게 대체 어떤 원리인 건지, 다른 모델과 어떤것이 다른건지 궁금해서 정리해보며 공부하도록 하겠다.\r\n개인적으로 공부한 것을 요약한 것이기에 틀린점이 있을 수 있으며, 사용된 이미지는 직접 제작한 것이 아니라 reference(주로 https://github.com/hkproj/transformer-from-scratch-notes)에서 찾은 이미지들로 구성되어 있음을 미리 밝힌다.\r\n\r\n# RNN과의 차이점\r\n기존 자연어 처리에는 RNN 방식이 주로 사용됨. RNN은 전체 입력에서 i개의 token들을 입력으로 사용해 결과물 token들을 출력하고, 다시 다음 time step에서 i개의 입력 token들을 활용해 결과를 출력하는 과정을 모든 입력을 처리할 때까지 반복하게 됨. 이 과정에서 모델의 hidden state가 변화하며 이전의 정보를 현재에 token 반영함\r\n<center>\r\n<img src=\"media/rnn.png\" width=\"80%\" title=\"rnn-workflow\"/>\r\n</center>\r\n\r\n-> for 문으로 전체 입력 토큰들을 각 time step마다 일정 수만큼 순회하며 결과물을 내놓는 것과 비슷한 방식\r\n\r\n따라서 RNN은 \r\n- for 문으로 입력 토큰들을 순회하며 결과물을 내놓는 방식으로 인하여 긴 입력에 대해 오랜 시간이 필요하게 됨\r\n- 이전 time step에서 입력받은 token들에서 나온 정보들이 step이 지남에 따라 기여도가 점차 적어지기에 활용하는 것이 어려움(=전체 context를 활용해 추론하는 것이 까다로움).\r\n\r\n이외에 긴 계산그래프로 인한 기울기 소멸문제 등도 있음\r\n\r\n-> 위 문제들을 해결하고자 transformer를 활용한 방법이 제시됨\r\n\r\n# Transformer\r\n<center>\r\n<img src=\"media/transformer.png\" width=\"50%\" title=\"transformer-workflow\"/>\r\n</center>\r\n\r\n## input embedding\r\ninput data가 입력되기 시작하는 input embedding 지점부터 살펴보자. 각 입력 문장들은 1~n개의 단어를 기준으로 tokenize 된다 (여기서는 1개 단어로 tokenize되는 경우만 고려). 그리고 각 token은 고유한 ID를 갖고 있으며, 해당 ID가 길이 512인 vector에 할당된다. 이때 이 vector값이 단어의 의미를 담는다고 볼 수 있고, vector의 값은 모델 학습을 통해 조정된다. 결론적으로 input embedding은 각 token으로부터 vector로의 mapping이다.\r\n<center>\r\n<img src=\"media/input_embedding.png\" width=\"80%\" title=\"input-embedding-process\"/>\r\n</center>\r\n\r\n## positional encoding\r\n각 token에 대해 input embedding이 진행되면  positional encoding이 각 input embedding에 더해진다. 이는 문장에서 각 token들의 위치에 따른 정보를 추가해주는 것으로 \"나는 귀여운 고양이를 가지고 있다\"와 같은 문장에서 각 token의 위치를 모델이 인식하도록 해준다.\r\n\r\npositional encoding은 각 문장에서 token의 순서와 각 token별 embedding vector의 각 원소의 순서 depth에 따른 함수이다. 이때 depth의 홀짝성에 따라 적용되는 함수의 형태가 약간 다르다.\r\n\r\n$$PE(pos, depth)=\r\n\\begin{cases}\r\n\\sin\\frac{pos}{10000^\\frac{2i}{d_{model}}},\\;if\\;depth = 2i\\\\\r\n\\cos\\frac{pos}{10000^\\frac{2i}{d_{model}}},\\;if\\;depth = 2i+1\r\n\\end{cases}$$\r\n\r\n<center>\r\n<img src=\"media/positional_encoding.png\" width=\"80%\" title=\"positional-encoding-process\"/>\r\n</center>\r\n\r\n주의할 것은 이 positional encoding은 문장이나 token의 내용과는 전혀 상관이 없으며, 오직 위치에 따라 값이 정해지는 개별적인 함수라는 것이다. 따라서 positional encoding은 단 한번 계산 이후 모든 문장에 대해 일괄적으로 적용 가능하다. 앞서 말했듯이 input embedding이 구해지면 각 vector에 같은 크기를 갖는 positional encoding이 더해진다.\r\n\r\npositional encoding이 sin과 cos 함수로 이루어진 이유는 pos와 depth(=2i, 2i+1)에 따라 함수를 시각화해보면 위치에 따라 여러 연속적인 패턴이 발생하는 것을 알 수 있는데, 모델이 이 패턴을 학습하기를 바랐기 때문이다.\r\n<center>\r\n<img src=\"media/pe_visualize.png\" width=\"80%\" title=\"positional-encoding-visualization\"/>\r\n</center>\r\n\r\n# Multi-Head Attention\r\nMulti-Head Attention을 다루기 전에 Self Attention 개념을 먼저 살펴보자\r\n\r\n## Self Attention\r\n\"나는 귀여운 고양이를 가지고 있다\"와 같은 문장에서 '나는'이라는 token은 '고양이'라는 token보다 '가지고 있다'라는 token과 더욱 연관되어 있고, 비슷하게 '귀여운'이라는 token은 '나는'보다 '고양이'와 서로 연관되어 있다. 이렇게 문장 내에서 단어간 연관성을 파악하기 위해 사용되는 것이 Self Attention이다. \r\n<center>\r\n<img src=\"media/self_attention.png\" width=\"80%\" title=\"self-attention-process\"/>\r\n</center>\r\n\r\nSelf Attention에서 Q, T, V는 전부 입력 문장에 대한 input embedding에 positional encoding을 더한 같은 행렬이다. 이때 적용되는 식은 아래와 같다. \r\n\r\n$$Attention(Q, T, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\r\n\r\n## reference\r\nUmar Jamil, \r\nAttention is all you need (Transformer) - Model explanation (including math), Inference and Training: https://www.youtube.com/watch?v=bCz4OMemCcA\r\n\r\nhkproj, transformer-from-scratch-notes: https://github.com/hkproj/transformer-from-scratch-notes","postTitle":"transformer이란 무엇인가","postDate":"2024-04-20","postAuthor":"bill0077","category":"transformer","slug":"what-is-transformer","filePath":"post-contents/transformer/what-is-transformer.md","mediaPath":"post-contents/transformer/media"}},"staticQueryHashes":[],"slicesMap":{}}